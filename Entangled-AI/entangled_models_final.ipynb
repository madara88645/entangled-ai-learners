{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94216e10",
   "metadata": {},
   "source": [
    "# ðŸ§  Entangled AI Models â€” CNN + MLP  \n",
    "This notebook demonstrates a proof-of-concept for *entangled learning* between heterogeneous models (a CNN and an MLP).  \n",
    "Each model independently learns a classification task, but their predictions are softly synchronized through a KL-based entangled loss component.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7081880",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56c74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten and normalize\n",
    "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5754988",
   "metadata": {},
   "source": [
    "## 2. Define CNN and MLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e74dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\TR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Reshape\n",
    "\n",
    "# Model A: CNN\n",
    "model_A = Sequential([\n",
    "    Reshape((28, 28, 1), input_shape=(784,)),\n",
    "    Conv2D(16, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model B: MLP\n",
    "model_B = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc6030",
   "metadata": {},
   "source": [
    "## 3. Define Entangled Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee005e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import KLDivergence, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "def compute_entangled_loss(y_true, y_pred_self, y_pred_other, base_loss_fn=None, entangle_weight=0.01):\n",
    "    if base_loss_fn is None:\n",
    "        base_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    kl_div = tf.keras.losses.KLDivergence()\n",
    "    base_loss = base_loss_fn(y_true, y_pred_self)\n",
    "    entangled_part = kl_div(y_pred_self, y_pred_other)\n",
    "    return base_loss + entangle_weight * entangled_part\n",
    "\n",
    "\n",
    "optimizer_A = Adam(learning_rate=0.01)\n",
    "optimizer_B = Adam(learning_rate=0.01)\n",
    "\n",
    "model_A.compile(optimizer=optimizer_A, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model_B.compile(optimizer=optimizer_B, loss=loss_fn, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81049a91",
   "metadata": {},
   "source": [
    "## 4. Train Both Models with Entangled Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1893c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 30\n",
    "def get_lambda(epoch, max_epochs, max_lambda=0.05):\n",
    "    return (epoch / max_epochs) * max_lambda\n",
    "\n",
    "lambda_history = []\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(100).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    Î» = get_lambda(epoch, max_epochs=30, max_lambda=0.05)\n",
    "    lambda_history.append(Î»)\n",
    "    print(f\"Epoch {epoch+1}: Î» = {Î»:.4f}\")\n",
    "      \n",
    "    epoch_loss_A, epoch_loss_B = [], []\n",
    "    correct_A, correct_B = 0, 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        # Model B'nin Ã§Ä±ktÄ±sÄ±nÄ± al\n",
    "        last_output_B = model_B(x_batch, training=False)\n",
    "        last_output_A = model_A(x_batch, training=False)\n",
    "        # === A eÄŸitimi ===\n",
    "        with tf.GradientTape() as tape_A:\n",
    "            pred_A = model_A(x_batch, training=True)\n",
    "            loss_A = compute_entangled_loss(\n",
    "            y_true=y_batch,\n",
    "            y_pred_self=pred_A,\n",
    "            y_pred_other=last_output_B,\n",
    "            base_loss_fn=loss_fn,\n",
    "            entangle_weight=Î»\n",
    "            )\n",
    "        grads_A = tape_A.gradient(loss_A, model_A.trainable_weights)\n",
    "        optimizer_A.apply_gradients(zip(grads_A, model_A.trainable_weights))\n",
    "        epoch_loss_A.append(loss_A.numpy())\n",
    "\n",
    "        # === B eÄŸitimi ===\n",
    "        with tf.GradientTape() as tape_B:\n",
    "            pred_B = model_B(x_batch, training=True)\n",
    "            loss_B = compute_entangled_loss(\n",
    "            y_true=y_batch,\n",
    "            y_pred_self=pred_B,\n",
    "            y_pred_other=last_output_A,\n",
    "            base_loss_fn=loss_fn,\n",
    "            entangle_weight=Î»\n",
    "            )\n",
    "        grads_B = tape_B.gradient(loss_B, model_B.trainable_weights)\n",
    "        optimizer_B.apply_gradients(zip(grads_B, model_B.trainable_weights))\n",
    "        epoch_loss_B.append(loss_B.numpy())\n",
    "\n",
    "        # Accuracy hesabÄ±\n",
    "        correct_A += np.sum(tf.argmax(pred_A, axis=1).numpy() == tf.argmax(y_batch, axis=1).numpy())\n",
    "        correct_B += np.sum(tf.argmax(pred_B, axis=1).numpy() == tf.argmax(y_batch, axis=1).numpy())\n",
    "        total += x_batch.shape[0]\n",
    "\n",
    "    print(f\"Model A - Loss: {np.mean(epoch_loss_A):.4f}, Accuracy: {correct_A/total:.4f}\")\n",
    "    print(f\"Model B - Loss: {np.mean(epoch_loss_B):.4f}, Accuracy: {correct_B/total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef86479",
   "metadata": {},
   "source": [
    "### Final Results\n",
    "\n",
    "- **Model A (CNN):**\n",
    "  - Final Accuracy: 99.64%\n",
    "  - Final Loss: 0.0318\n",
    "\n",
    "- **Model B (MLP):**\n",
    "  - Final Accuracy: 98.74%\n",
    "  - Final Loss: 0.0659\n",
    "\n",
    "- **Lambda Scheduler:**\n",
    "  - Linear increase from 0 to 0.05 over 30 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0a1f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "In this notebook, we demonstrated an entangled learning setup using a CNN and an MLP on the MNIST dataset.  \n",
    "Each model was trained independently, but their outputs were softly synchronized using a dynamic KL-divergence-based loss function.  \n",
    "This architecture shows promising potential for distributed and privacy-preserving collaborative learning.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
